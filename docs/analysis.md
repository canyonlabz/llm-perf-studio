# üìä LLM KPI Analysis: Data Sources, Equations & Interpretation

## Overview

This framework standardizes large language model (LLM) performance metrics across both Ollama and OpenAI JMeter scripts. By unifying the output format, users can accurately benchmark different LLM backends‚Äîeven when API limitations require approximations. Below we explain the metrics schema, show equations for key inference KPIs, and compare CSV field outputs from both Ollama (local, more granular) and OpenAI (remote, less granular).

***

## üìë Metrics Data Schema

Both JMeter scripts output CSV files with the following standardized columns:

- `timestamp`: When the request was made.
- `model_name`: Backend/model identifier.
- `question_number`: Prompt/test identifier.
- `prompt_tokens`: Number of tokens in user prompt.
- `completion_tokens`: Number of tokens generated in response.
- `total_tokens`: Total tokens (prompt + completion).
- `eval_count`: Alias for `completion_tokens`.
- `total_duration_ms`: Total duration (milliseconds) for request to complete.
- `load_duration_ms`: Time to set up model/session.
- `prompt_eval_duration_ms`: Server-side computation for evaluating prompt.
- `eval_duration_ms`: Duration spent generating output tokens.
- `elapsed_ms`, `latency_ms`, `connect_time_ms`: Additional timing fields from JMeter.
- `allThreads`: JMeter thread count.

***

## ‚öñÔ∏è Comparison Table: Ollama vs. OpenAI CSV Fields

| Field Name | Ollama Output Example | OpenAI Output Example | Description |
| :-- | :-- | :-- | :-- |
| `prompt_tokens` | Prompt tokens from model (local count) | Extracted from API response | Number of tokens in input prompt |
| `completion_tokens` | Output tokens generated by model | Output tokens from API response | Number of tokens produced in completion |
| `total_tokens` | Sum of prompt and completion tokens | Sum of prompt and completion tokens | All tokens involved in transaction |
| `eval_count` | `completion_tokens` | `completion_tokens` | Number of generated output tokens |
| `total_duration_ms` | True processing time (converted from nanoseconds) | Client-observed elapsed time | Time for request/response cycle in milliseconds |
| `load_duration_ms` | Model load/setup time (nanoseconds ‚Üí ms) | TCP connect time (network setup) | Time spent loading model (local) or connecting (API) |
| `prompt_eval_duration_ms` | Model‚Äôs prompt evaluation duration (ns ‚Üí ms) | Latency - connect time (approximation) | Time spent evaluating the prompt or waiting on server |
| `eval_duration_ms` | Output token generation time (ns ‚Üí ms) | Elapsed - latency (approximation) | Time actually spent generating tokens post-first byte |
| `elapsed_ms` | Total elapsed per JMeter | Total elapsed per JMeter | Measured total time; reference only |
| `latency_ms` | Time to first byte | Time to first byte (latency) | Measures network or backend speed |
| `connect_time_ms` | TCP connection time | TCP connection time | Time to establish network connection |
| `allThreads` | JMeter active threads | JMeter active threads | Used for load testing |

*Where:*

- Ollama Groovy PostProcessor outputs via `/api/generate`
- OpenAI Groovy PostProcessor outputs via `/v1/chat/completions`

***

## üìê KPI Equations \& Explanations

All KPI calculations are performed using the standardized fields above.

### **üü£ Time to First Token (TTFT):**

$$
TTFT = load\_duration\_ms + prompt\_eval\_duration\_ms
$$

*Measures the time from the start of the request until the first output token is produced.*

### **üîµ Tokens Per Second (TPS):**

$$
TPS = \frac{eval\_count}{total\_duration\_ms / 1000}
$$

*Represents generation speed‚Äîthe number of output tokens produced per second.*

### **üü¢ Time Per Output Token (TPOT):**

$$
TPOT = \frac{total\_duration\_ms - TTFT}{eval\_count}
$$

*Average time spent generating each token, excluding setup and prompt evaluation.*

***

## üõ†Ô∏è Backend Differences \& Limitations

- **Ollama:** Provides precise timings and token counts, so KPIs like TPOT, TTFT, and TPS closely reflect real hardware/model performance.
- **OpenAI:** Relies on client-observed timings; fields like TTFT and eval_duration_ms are approximated, so KPIs are less precise and primarily useful for relative comparison. Network delay, server batching, and queuing may inflate timing values, especially compared to local models.

***

## üìö DeepEval Analysis

_Work in progress: This section will describe how accuracy/pass rate is analyzed using the DeepEval test suite across both backends._
