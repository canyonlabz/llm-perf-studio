# LLM Performance Test Studio

> **âš ï¸ This project is under active development. Features, modules, and documentation may change frequently. Use at your own risk and please report any issues or suggestions!**

A comprehensive framework for load testing Large Language Models (LLMs) with integrated quality assessment using JMeter and DeepEval.

## ğŸš€ Overview

This framework combines traditional load testing with AI-specific quality metrics to provide a complete performance evaluation of LLM services. It supports testing both local models (via Ollama) and cloud-based APIs (OpenAI) while measuring both performance and response quality under load.

## âœ¨ Key Features

- **Dual Testing Approach**: Load testing with JMeter + Quality assessment with DeepEval
- **Multi-LLM Support**: Ollama (local models) and OpenAI API compatibility
- **Real-time Monitoring**: Live dashboard with performance metrics and logs
- **Quality Under Load**: Track accuracy degradation as load increases
- **Comprehensive Metrics**: TTFT, TPOT, TPS, and traditional HTTP metrics
- **Interactive UI**: Streamlit-based dashboard for test management
- **RAG Support**: Test Retrieval-Augmented Generation workflows

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit UI  â”‚    â”‚  JMeter Engine  â”‚    â”‚  LLM Service    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Test Config   â”‚â”€â”€â–¶â”‚ â€¢ Load Testing  â”‚â”€â”€â”€â–¶â”‚ â€¢ Ollama/OpenAI â”‚
â”‚ â€¢ Monitoring    â”‚    â”‚ â€¢ Metrics       â”‚    â”‚ â€¢ Local/Cloud   â”‚
â”‚ â€¢ Results       â”‚    â”‚ â€¢ Logging       â”‚    â”‚ â€¢ RAG Enabled   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                      â”‚
         â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  DeepEval QA    â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚                 â”‚
                       â”‚ â€¢ Correctness   â”‚
                       â”‚ â€¢ Quality Score â”‚
                       â”‚ â€¢ Category Test â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

- **Python 3.12+**
- **Apache JMeter 5.6.3+**
- **Java OpenJDK 22+** (*for JMeter*)
- **Ollama** (*optional: for local model testing*)
- **OpenAI API Key** (*optional: for OpenAI testing*)

## ğŸ› ï¸ Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/canyonlabz/llm-perf-studio.git
   cd llm-perf-studio
   ```

2. **Install Python dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Install JMeter:**
   - Download from [Apache JMeter](https://jmeter.apache.org/download_jmeter.cgi)
   - Extract locally to folder of your choice. Example: `C:\opt\apache-jmeter-5.6.3\` (Windows) or `/opt/apache-jmeter-5.6.3/` (Linux/Mac)

4. **Configure environment:**
   ```bash
   cp config.yaml.example config.yaml
   # Edit config.yaml with your settings
   ```
   You can create a `config.windows.yaml` or `config.mac.yaml` file depending on your operating system.
   The OS-specific YAML file will override the default `config.yaml` settings.
5. **Set up Ollama (optional):**
   ```bash
   # Install Ollama
   curl -fsSL https://ollama.ai/install.sh | sh
   
   # Pull a model
   ollama pull llama3.2:1b
   ```

## âš™ï¸ Configuration

### Main Configuration (`~/config.yaml`)

> ğŸ’¡ **Note:** For a full breakdown of available configuration options and advanced usage, check out [docs/configuration.md](docs/configuration.md).

**Example:**
```yaml
ollama:
   base_url: "http://localhost:11434"
   model: "llama3.2:1b"
   timeout: 30
  
openai:
   model: "gpt-5-mini"
   timeout: 30

jmeter:
  bin_path: "C:/{{jmeter_path}}/apache-jmeter-5.6.3/bin"

deepeval:
  evaluator_model: "gpt-4"
  correctness_threshold: 0.7
```

### Environment Variables

```bash
# .env file
OPENAI_API_KEY=your_openai_api_key_here
```

## ğŸš€ Quick Start

1. **Start the dashboard:**
   ```bash
   python app.py
   ```

2. **Configure your test:**
   - Select LLM service (Ollama/OpenAI)
   - Set load parameters (users, duration, ramp-up)
   - Choose test data and RAG settings

3. **Run performance test:**
   - Click "Start JMeter Test"
   - Monitor real-time logs and metrics
   - View live performance charts

4. **Quality assessment:**
   - Navigate to "Quality Assessment" tab
   - Run DeepEval analysis on test responses
   - Review quality scores and categorized results

## ğŸ“Š Metrics & KPIs

### Performance Metrics
- **Response Time**: Average, Min, Max, 90th percentile
- **Throughput**: Requests per second
- **Error Rate**: Failed requests percentage
- **Concurrent Users**: Virtual user load over time

### LLM-Specific Metrics
- **TTFT**: Time to First Token
- **TPOT**: Time Per Output Token  
- **TPS**: Tokens Per Second
- **Token Counts**: Input/Output token statistics

### Quality Metrics
- **Correctness Score**: DeepEval accuracy assessment
- **Category Performance**: Domain-specific quality tracking
- **Quality Under Load**: Accuracy vs load correlation

## ğŸ“ Project Structure

```
llm-perf-studio/
â”‚   app.py
â”‚   config.yaml
â”‚   LICENSE
â”‚   README.md
â”‚   requirements.txt
â”‚
â”œâ”€â”€â”€data
â”‚       ISTQB_CT-AI_SampleExam-Answers_v1.0.pdf
â”‚       ISTQB_CT-AI_SampleExam-Questions_v1.0.pdf
â”‚
â”œâ”€â”€â”€docker
â”‚       docker-compose.yml
â”‚
â”œâ”€â”€â”€docs
|       analysis.md
|       docker.md
â”‚       kpis.md
â”‚
â”œâ”€â”€â”€jmeter
â”‚   â”‚   llm-ollama.jmx
â”‚   â”‚   llm-openai.jmx
â”‚   â”‚
â”‚   â”œâ”€â”€â”€testdata_csv
â”‚   â”‚       environment_ollama.csv
|   |       environment_openai.csv
â”‚   â”‚       README.md
â”‚   â”‚
â”‚   â””â”€â”€â”€testdata_json
â”‚           ISTQB_Final_Questions_Answers.json
â””â”€â”€â”€src
    â”‚
    â”œâ”€â”€â”€services
    â”‚       chat_service.py     (Class for LLM chatbot)
    â”‚
    â”œâ”€â”€â”€tools
    â”‚   â”‚   deepeval_assessment.py              (Agent Tool for DeepEval quality assessment)
    â”‚   â”‚   jmeter_executor.py                  (Agent tool for JMeter test execution)
    â”‚   â””   llm_kpi_calculations.py             (Agent Tool for calculating LLM KPI metrics)
    â”‚
    â”œâ”€â”€â”€ui
    â”‚   â”‚   page_body_*.py  (page body rendering)
    â”‚   â”‚   page_header.py  (page headers rendering)
    â”‚   â”‚   page_styles.py  (page CSS style rendering)
    â”‚   â”‚   page_title.py   (page title rendering)
    â”‚   â”‚   page_utils.py   (page utility functions)
    â”‚   â”‚   streamlit_ui.py (page rendering function for all components)
    â”‚   â”‚   ui_handlers.py  (page UI handler functions)
    â”‚   â”‚
    â”‚   â””â”€â”€â”€nav_pages
    â”‚       page_*.py       (Streamlit Pages)
    â”‚
    â””â”€â”€â”€utils
            Common Utilities
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/new-feature`)
3. Commit changes (`git commit -am 'Add new feature'`)
4. Push to branch (`git push origin feature/new-feature`)
5. Create a Pull Request

## ğŸ“ License

This project is licensed under the BSD-3-Clause License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- **Documentation**: Check the `docs/` directory
- **Issues**: Report bugs via GitHub Issues
- **Discussions**: Use GitHub Discussions for questions

## ğŸ”® Roadmap

- [ ] Support for additional LLM providers (Anthropic, Google, etc.)
- [ ] Full RAG support for custom datasets
- [ ] Docker-based containerization for cross-platform, OS-agnostic deployment
- [ ] Additional DeepEval quality metrics
- [X] De-couple LLM calculations from JMeter to Python tools. 

## ğŸ“š Related Projects

- [Apache JMeter](https://jmeter.apache.org/) - Load and performance testing tool for web applications and other services.
- [DeepEval](https://github.com/confident-ai/deepeval) - LLM evaluation framework and platform for testing and evaluating large language models (LLMs).
- [Ollama](https://ollama.ai/) - Ollama is an open-source platform that lets you run large language models on your device.
- [ChromaDB](https://docs.trychroma.com/docs/overview/introduction) - Open-source vector database tailored to applications with large language models.
- [OpenWebUI](https://docs.openwebui.com/) - Extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.
- [Streamlit](https://streamlit.io/) - Open-source Python framework for data scientists and AI/ML engineers to deliver interactive data apps.
- [Docker](https://www.docker.com/) - Docker is a tool that helps developers build, share, run, and verify applications using containers.
- [Unstructured](https://github.com/Unstructured-IO/unstructured-api) - Core library for partitioning, cleaning, and chunking 25+ documents types for LLM applications and connecting to source and destination data source.

## Author âœï¸

**Jason Smallcanyon | [CanyonLabz, LLC](https://canyonlabz.com/)**

---

**Built with â¤ï¸ for the LLM performance testing community**
